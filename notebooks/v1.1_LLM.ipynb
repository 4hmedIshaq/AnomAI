{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4872a8a0",
   "metadata": {},
   "source": [
    "### Setup Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706e8c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0636626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project source code to path for grabbing functions\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e82b2",
   "metadata": {},
   "source": [
    "#### Import all security rules and llm context helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44542503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from security_rules_build import combined_rules, build_llm_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba7f7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected, total rows: 1920624\n"
     ]
    }
   ],
   "source": [
    "#connect to the database\n",
    "con = duckdb.connect(\"../data/logs.duckdb\")\n",
    "#sample check\n",
    "row_count = con.execute(\"SELECT COUNT(*) FROM logs\").fetchone()[0]\n",
    "print(\"Connected, total rows:\", row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c128be72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['failed_logins', 'failed_by_ip', 'brute_force', 'priv_esc', 'account_lockout', 'log_cleared', 'susp_linux', 'susp_win', 'rdp_logins', 'ssh_root_logins', 'failed_count_windows'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Laying out the dictionary keys which are the names of each security rule dataframe. \n",
    "results = run_all_rules(con)\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2661d0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§© Context summary ready for LLM.\n",
      "\n",
      "=== Security Detection Summary ===\n",
      "\n",
      "ðŸ”¹ FAILED_LOGINS (Windows: 63, Ubuntu: 16597)\n",
      "Total: 16660 events\n",
      "  AGENT_NAME                                                                                      Content          src_ip        dst_ip           entity_id    City Country                date day hour minute second           timestamp\n",
      "UBUNTU_AGENT sshd[2756135]: Failed password for invalid user site from 185.213.165.107 port 48410 ssh2... 185.213.165.107 172.21.35.175 Nisir@172.21.35.175  Tehran    Iran 2025-05-15 09:25:56   3    9     25     56 2025-05-15 09:25:56\n",
      "UBUNTU_AGENT                 sshd[2653867]: Failed password for root from 218.92.0.177 port 40367 ssh2...    218.92.0.177 172.21.35.175 Nisir@172.21.35.175 Unknown   China 2025-05-14 10:14:47   2   10     14     47 2025-05-14 10:14:47\n",
      "\n",
      "ðŸ”¹ FAILED_BY_IP\n",
      "Total: 9 events\n",
      "      src_ip attempts\n",
      "172.20.82.21       55\n",
      " 172.20.0.39       33\n",
      "\n",
      "ðŸ”¹ BRUTE_FORCE\n",
      "Total: 12 events\n",
      "      src_ip                    entity_id failed_attempts\n",
      "172.20.82.21 DESKTOP-Q0KDN51@172.20.82.21           93885\n",
      "172.20.82.43           ubuntu@172.20.0.42           16352\n",
      "\n",
      "ðŸ”¹ PRIV_ESC (Windows: 2931, Ubuntu: 56062)\n",
      "Total: 58993 events\n",
      "  AGENT_NAME                                                                                  Content      src_ip      dst_ip          entity_id    City Country                date day hour minute second           timestamp\n",
      "UBUNTU_AGENT CRON[1427966]: pam_unix(cron:session): session opened for user root(uid=0) by (uid=0)... 172.20.0.42 172.20.0.42 ubuntu@172.20.0.42 Unknown Unknown 2024-12-14 03:17:01   5    3     17      1 2024-12-14 03:17:01\n",
      "UBUNTU_AGENT  CRON[472954]: pam_unix(cron:session): session opened for user root(uid=0) by (uid=0)... 172.20.0.42 172.20.0.42 ubuntu@172.20.0.42 Unknown Unknown 2025-05-13 20:17:01   1   20     17      1 2025-05-13 20:17:01\n",
      "\n",
      "ðŸ”¹ ACCOUNT_LOCKOUT (Windows: 81, Ubuntu: 0)\n",
      "Total: 81 events\n",
      "   AGENT_NAME                                                                                                    Content       src_ip       dst_ip             entity_id    City Country                date day hour minute second           timestamp\n",
      "WINDOWS_AGENT Microsoft-Windows-Security-Auditing\\tEvent ID: 4656\\tType: 8\\tDescription: A handle to an object was re... 172.20.82.33 172.20.82.33 Lemonade@172.20.82.33 Unknown Unknown 2025-05-14 09:38:13   2    9     38     13 2025-05-14 09:38:13\n",
      "WINDOWS_AGENT Microsoft-Windows-Security-Auditing\\tEvent ID: 4658\\tType: 8\\tDescription: The handle to an object was ... 172.20.82.33 172.20.82.33 Lemonade@172.20.82.33 Unknown Unknown 2025-05-14 10:28:14   2   10     28     14 2025-05-14 10:28:14\n",
      "\n",
      "ðŸ”¹ LOG_CLEARED (Windows: 66, Ubuntu: 0)\n",
      "Total: 66 events\n",
      "   AGENT_NAME                                                                                                     Content       src_ip       dst_ip                    entity_id    City Country                date day hour minute second           timestamp\n",
      "WINDOWS_AGENT  Microsoft-Windows-Security-Auditing\\tEvent ID: 4656\\tType: 8\\tDescription: A handle to an object was re... 172.20.82.33 172.20.82.33        Lemonade@172.20.82.33 Unknown Unknown 2025-05-14 09:31:09   2    9     31      9 2025-05-14 09:31:09\n",
      "WINDOWS_AGENT PowerShell\\tEvent ID: 600\\tType: 4\\tDescription: Provider \"Registry\" is Started.   Details:  \\tProviderN... 172.20.82.21 172.20.82.21 DESKTOP-Q0KDN51@172.20.82.21 Unknown Unknown 2025-05-13 13:31:53   1   13     31     53 2025-05-13 13:31:53\n",
      "\n",
      "ðŸ”¹ SUSP_LINUX (Windows: 0, Ubuntu: 24)\n",
      "Total: 24 events\n",
      "  AGENT_NAME                                                                                                 Content      src_ip      dst_ip          entity_id    City Country                date day hour minute second           timestamp\n",
      "UBUNTU_AGENT ansible-ansible.legacy.command[3585]: Invoked with chdir=/usr/share/wazuh-indexer/bin/ _raw_params=.... 172.20.0.47 172.20.0.47 debian@172.20.0.47 Unknown Unknown 2025-04-08 04:23:03   1    4     23      3 2025-04-08 04:23:03\n",
      "UBUNTU_AGENT ansible-ansible.legacy.command[59285]: Invoked with chdir=/usr/share/wazuh-indexer/bin/ _raw_params=... 172.20.0.47 172.20.0.47 debian@172.20.0.47 Unknown Unknown 2025-04-08 04:56:40   1    4     56     40 2025-04-08 04:56:40\n",
      "\n",
      "ðŸ”¹ SUSP_WIN (Windows: 32, Ubuntu: 0)\n",
      "Total: 32 events\n",
      "   AGENT_NAME                                                                                                     Content       src_ip       dst_ip                    entity_id    City Country                date day hour minute second           timestamp\n",
      "WINDOWS_AGENT PowerShell\\tEvent ID: 600\\tType: 4\\tDescription: Provider \"Function\" is Started.   Details:  \\tProviderN... 172.20.82.21 172.20.82.21 DESKTOP-Q0KDN51@172.20.82.21 Unknown Unknown 2025-04-25 09:48:44   4    9     48     44 2025-04-25 09:48:44\n",
      "WINDOWS_AGENT  PowerShell\\tEvent ID: 403\\tType: 4\\tDescription: Engine state is changed from Available to Stopped.   D... 172.20.82.21 172.20.82.21 DESKTOP-Q0KDN51@172.20.82.21 Unknown Unknown 2025-05-14 12:47:45   2   12     47     45 2025-05-14 12:47:45\n",
      "\n",
      "ðŸ”¹ RDP_LOGINS: No records found.\n",
      "\n",
      "ðŸ”¹ SSH_ROOT_LOGINS (Windows: 0, Ubuntu: 30819)\n",
      "Total: 30819 events\n",
      "  AGENT_NAME                                                                                                 Content        src_ip        dst_ip           entity_id              City Country                date day hour minute second           timestamp\n",
      "UBUNTU_AGENT sshd[2567023]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rho... 172.21.35.175 172.21.35.175 Nisir@172.21.35.175           Unknown Unknown 2025-05-13 12:24:21   1   12     24     21 2025-05-13 12:24:21\n",
      "UBUNTU_AGENT       sshd[2757072]: Connection closed by authenticating user root 159.89.14.55 port 36946 [preauth]...  159.89.14.55 172.21.35.175 Nisir@172.21.35.175 Frankfurt am Main Germany 2025-05-15 09:38:29   3    9     38     29 2025-05-15 09:38:29\n",
      "\n",
      "ðŸ”¹ FAILED_COUNT_WINDOWS\n",
      "Total: 1 events\n",
      "failed_attempts\n",
      "            109\n",
      "\n",
      "Source: Aggregated detections from Windows and Ubuntu systems.\n"
     ]
    }
   ],
   "source": [
    "context_summary = build_llm_context(results, examples=2)\n",
    "print(context_summary)  # preview only first 1500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eea9ae",
   "metadata": {},
   "source": [
    "### Setup and keep local LLM ready "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6436bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the model path into a variable\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/Llama-2-13B-chat-GGUF\",\n",
    "    filename=\"llama-2-13b-chat.Q5_K_M.gguf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17cbe3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4070, compute capability 8.9, VMM: yes\n",
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4070) - 11094 MiB free\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from C:\\Users\\Ishaq\\.cache\\huggingface\\hub\\models--TheBloke--Llama-2-13B-chat-GGUF\\snapshots\\4458acc949de0a9914c3eab623904d4fe999050a\\llama-2-13b-chat.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q5_K:  241 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q5_K - Medium\n",
      "print_info: file size   = 8.60 GiB (5.67 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 5120\n",
      "print_info: n_layer          = 40\n",
      "print_info: n_head           = 40\n",
      "print_info: n_head_kv        = 40\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 5120\n",
      "print_info: n_embd_v_gqa     = 5120\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 13824\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 13B\n",
      "print_info: model params     = 13.02 B\n",
      "print_info: general.name     = LLaMA v2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  33 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  34 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  35 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  36 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  37 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  38 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  39 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  40 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q5_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
      "load_tensors: offloading 40 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 41/41 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size =  8694.21 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   107.42 MiB\n",
      "...................................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "set_abort_callback: call\n",
      "llama_context:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_context: n_ctx = 4096\n",
      "llama_context: n_ctx = 4096 (padded)\n",
      "init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1\n",
      "init: layer   0: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer   1: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer   2: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer   3: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer   4: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer   5: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer   6: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer   7: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer   8: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer   9: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  10: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  11: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  12: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  13: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  14: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  15: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  16: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  17: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  18: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  19: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  20: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  21: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  22: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  23: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  24: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  25: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  26: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  27: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  28: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  29: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  30: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  31: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  32: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  33: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  34: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  35: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  36: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  37: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  38: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init: layer  39: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120, dev = CUDA0\n",
      "init:      CUDA0 KV buffer size =  3200.00 MiB\n",
      "llama_context: KV self size  = 3200.00 MiB, K (f16): 1600.00 MiB, V (f16): 1600.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 2\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:      CUDA0 compute buffer size =   368.00 MiB\n",
      "llama_context:  CUDA_Host compute buffer size =    18.01 MiB\n",
      "llama_context: graph nodes  = 1366\n",
      "llama_context: graph splits = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA 13B model loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA : ARCHS = 890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '5120', 'llama.block_count': '40', 'llama.feed_forward_length': '13824', 'llama.attention.head_count': '40', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '17', 'llama.attention.head_count_kv': '40', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "# Load the LLM\n",
    "from llama_cpp import Llama, llama_print_system_info\n",
    "\n",
    "llm = Llama(\n",
    "    model_path= model_path,\n",
    "    n_ctx=4096,\n",
    "    n_threads=12,\n",
    "    n_gpu_layers=42,\n",
    "    n_batch=512\n",
    ")\n",
    "\n",
    "#llama_print_system_info()\n",
    "print(\"LLaMA 13B model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35146679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '5120', 'llama.block_count': '40', 'llama.feed_forward_length': '13824', 'llama.attention.head_count': '40', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '17', 'llama.attention.head_count_kv': '40', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n"
     ]
    }
   ],
   "source": [
    "print(llm.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ff9ff",
   "metadata": {},
   "source": [
    "#### First test using LLM context helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88d59885",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =f\"\"\"\n",
    "You are a SOC (Security Operations Center) analyst.\n",
    "Analyze the following structured SIEM detections.\n",
    "\n",
    "For each rule, describe:\n",
    "1. What the detected activity means,\n",
    "2. The potential threat or attack pattern it indicates,\n",
    "3. Recommended actions oar mitigations.\n",
    "\n",
    "Here are the detections:\n",
    "{context_summary[:7000]}\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a152ff96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   17187.48 ms\n",
      "llama_perf_context_print: prompt eval time =   17184.73 ms /  2363 tokens (    7.27 ms per token,   137.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =   77145.54 ms /   699 runs   (  110.37 ms per token,     9.06 tokens per second)\n",
      "llama_perf_context_print:       total time =   94890.24 ms /  3062 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA Analysis:\n",
      "  As a SOC analyst, I have analyzed the provided SIEM detections and recommend the following actions and mitigations based on the detected activity:\n",
      "\n",
      "1. ðŸ”¹ Rule: FAILED_LOGINS (Windows: 63, Ubuntu: 16597)\n",
      "Total records: 16660\n",
      "\n",
      "Potential threat or attack pattern: Brute-force attacks or password guessing attempts.\n",
      "\n",
      "Recommended actions and mitigations:\n",
      "\n",
      "a. Implement strong password policies and enforce password complexity requirements.\n",
      "\n",
      "b. Enable multi-factor authentication (MFA) to add an extra layer of security.\n",
      "\n",
      "c. Restrict login attempts to prevent excessive attempts from the same IP address.\n",
      "\n",
      "d. Monitor and review failed login attempts regularly to detect and respond to potential attacks.\n",
      "\n",
      "2. ðŸ”¹ Rule: FAILED_BY_IP\n",
      "Total records: 9\n",
      "\n",
      "Potential threat or attack pattern: Brute-force attacks or password guessing attempts from specific IP addresses.\n",
      "\n",
      "Recommended actions and mitigations:\n",
      "\n",
      "a. Block or restrict access from the identified IP addresses.\n",
      "\n",
      "b. Implement IP address blacklisting to prevent further attempts from these IPs.\n",
      "\n",
      "c. Review and update firewall rules to ensure they are correctly configured to block malicious traffic.\n",
      "\n",
      "3. ðŸ”¹ Rule: BRUTE_FORCE\n",
      "Total records: 12\n",
      "\n",
      "Potential threat or attack pattern: Brute-force attacks or password guessing attempts.\n",
      "\n",
      "Recommended actions and mitigations:\n",
      "\n",
      "a. Implement strong password policies and enforce password complexity requirements.\n",
      "\n",
      "b. Enable multi-factor authentication (MFA) to add an extra layer of security.\n",
      "\n",
      "c. Restrict login attempts to prevent excessive attempts from the same IP address.\n",
      "\n",
      "d. Monitor and review failed login attempts regularly to detect and respond to potential attacks.\n",
      "\n",
      "4. ðŸ”¹ Rule: PRIV_ESC (Windows: 2931, Ubuntu: 56062)\n",
      "Total records: 58993\n",
      "\n",
      "Potential threat or attack pattern: Privilege escalation attempts.\n",
      "\n",
      "Recommended actions and mitigations:\n",
      "\n",
      "a. Implement least privilege policies to minimize the risk of privilege escalation.\n",
      "\n",
      "b. Regularly review and update software and system configurations to ensure they are up-to-date and secure.\n",
      "\n",
      "c. Monitor and review system and application logs to detect and respond to potential privilege escalation attempts.\n",
      "\n",
      "5. ðŸ”¹ Rule: ACCOUNT_LOCKOUT (Windows: 81, Ubuntu: 0)\n",
      "Total records: 81\n",
      "\n",
      "Potential threat or attack pattern: Lockout attacks or password guessing attempts.\n",
      "\n",
      "Recommended actions and mitigations:\n",
      "\n",
      "a. Implement strong password policies and enforce password complexity requirements.\n",
      "\n",
      "b. Enable multi-factor authentication (MFA) to add an extra layer of security.\n",
      "\n",
      "c. Restrict login attempts to prevent excessive attempts from the same IP address.\n"
     ]
    }
   ],
   "source": [
    "response = llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert SOC() analyst.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_tokens=700,\n",
    "    temperature=0.4,\n",
    ")\n",
    "\n",
    "print(\"LLaMA Analysis:\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2b6636",
   "metadata": {},
   "source": [
    "### Adding raw data with context data for an output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a596e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sample = con.execute(\"SELECT * FROM logs LIMIT 50\").fetchdf()\n",
    "raw_sample[\"Content\"] = raw_sample[\"Content\"].astype(str).str.slice(0, 200) + \"...\"\n",
    "raw_text = raw_sample.to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f5d12bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "''''context_summary = build_llm_context(results, examples=3)\n",
    "\n",
    "context_full = f\"\"\"\n",
    "=== STRUCTURED SECURITY SUMMARY ===\n",
    "{context_summary[:5000]}\n",
    "\n",
    "=== RAW LOG SAMPLE ===\n",
    "{raw_text}\n",
    "\n",
    "Analyze both the structured detections and the raw log data together.\n",
    "Identify major threats, link raw evidence to detections, and describe what an SOC analyst should do next.\n",
    "\"\"\"\n",
    "'''\n",
    "\n",
    "context_summary = build_llm_context(results, examples=3)\n",
    "\n",
    "context_full = f\"\"\"\n",
    "=== RAW LOG SAMPLE ===\n",
    "{raw_text}\n",
    "\n",
    "Analyze the  the raw log data.\n",
    "Identify major threats, link raw evidence to detections, and describe what an SOC analyst should do next.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38b238ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Requested tokens (8744) exceed context window of 4096",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are a senior SOC analyst reviewing SIEM data.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_full\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m700\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ§  Full Analysis:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(response[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ishaq\\anaconda3\\envs\\llama_clean\\Lib\\site-packages\\llama_cpp\\llama.py:2001\u001b[39m, in \u001b[36mLlama.create_chat_completion\u001b[39m\u001b[34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs)\u001b[39m\n\u001b[32m   1963\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate a chat completion from a list of messages.\u001b[39;00m\n\u001b[32m   1964\u001b[39m \n\u001b[32m   1965\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1994\u001b[39m \u001b[33;03m    Generated chat completion or a stream of chat completion chunks.\u001b[39;00m\n\u001b[32m   1995\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1996\u001b[39m handler = (\n\u001b[32m   1997\u001b[39m     \u001b[38;5;28mself\u001b[39m.chat_handler\n\u001b[32m   1998\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._chat_handlers.get(\u001b[38;5;28mself\u001b[39m.chat_format)\n\u001b[32m   1999\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m llama_chat_format.get_chat_completion_handler(\u001b[38;5;28mself\u001b[39m.chat_format)\n\u001b[32m   2000\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2001\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllama\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2007\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2008\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2010\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2011\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2012\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2013\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2014\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2015\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2016\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2017\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2018\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2019\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2020\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2022\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2023\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2024\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2025\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2026\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2028\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2031\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ishaq\\anaconda3\\envs\\llama_clean\\Lib\\site-packages\\llama_cpp\\llama_chat_format.py:662\u001b[39m, in \u001b[36mchat_formatter_to_chat_completion_handler.<locals>.chat_completion_handler\u001b[39m\u001b[34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs, **kwargs)\u001b[39m\n\u001b[32m    657\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e), file=sys.stderr)\n\u001b[32m    658\u001b[39m         grammar = llama_grammar.LlamaGrammar.from_string(\n\u001b[32m    659\u001b[39m             llama_grammar.JSON_GBNF, verbose=llama.verbose\n\u001b[32m    660\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m completion_or_chunks = \u001b[43mllama\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    688\u001b[39m     tool_name = tool[\u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ishaq\\anaconda3\\envs\\llama_clean\\Lib\\site-packages\\llama_cpp\\llama.py:1835\u001b[39m, in \u001b[36mLlama.create_completion\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1833\u001b[39m     chunks: Iterator[CreateCompletionStreamResponse] = completion_or_chunks\n\u001b[32m   1834\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[32m-> \u001b[39m\u001b[32m1835\u001b[39m completion: Completion = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1836\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ishaq\\anaconda3\\envs\\llama_clean\\Lib\\site-packages\\llama_cpp\\llama.py:1271\u001b[39m, in \u001b[36mLlama._create_completion\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1268\u001b[39m     \u001b[38;5;28mself\u001b[39m._ctx.reset_timings()\n\u001b[32m   1270\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_tokens) >= \u001b[38;5;28mself\u001b[39m._n_ctx:\n\u001b[32m-> \u001b[39m\u001b[32m1271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1272\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRequested tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompt_tokens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exceed context window of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllama_cpp.llama_n_ctx(\u001b[38;5;28mself\u001b[39m.ctx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1273\u001b[39m     )\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_tokens <= \u001b[32m0\u001b[39m:\n\u001b[32m   1276\u001b[39m     \u001b[38;5;66;03m# Unlimited, depending on n_ctx.\u001b[39;00m\n\u001b[32m   1277\u001b[39m     max_tokens = \u001b[38;5;28mself\u001b[39m._n_ctx - \u001b[38;5;28mlen\u001b[39m(prompt_tokens)\n",
      "\u001b[31mValueError\u001b[39m: Requested tokens (8744) exceed context window of 4096"
     ]
    }
   ],
   "source": [
    "response = llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a senior SOC analyst reviewing SIEM data.\"},\n",
    "        {\"role\": \"user\", \"content\": context_full}\n",
    "    ],\n",
    "    max_tokens=700,\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "print(\"ðŸ§  Full Analysis:\\n\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a634582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =f\"\"\"\n",
    "You are a SOC (Security Operations Center) analyst.\n",
    "Analyze the following structured SIEM detections.\n",
    "\n",
    "For each rule, describe:\n",
    "1. What the detected activity means,\n",
    "2. The potential threat or attack pattern it indicates,\n",
    "3. Recommended actions oar mitigations.\n",
    "\n",
    "Here are the detections:\n",
    "{results}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e49acee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Requested tokens (17091) exceed context window of 4096",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are a senior SOC analyst reviewing SIEM data.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m700\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFull Analysis:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(response[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ishaq\\anaconda3\\envs\\llama_clean\\Lib\\site-packages\\llama_cpp\\llama.py:2001\u001b[39m, in \u001b[36mLlama.create_chat_completion\u001b[39m\u001b[34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs)\u001b[39m\n\u001b[32m   1963\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate a chat completion from a list of messages.\u001b[39;00m\n\u001b[32m   1964\u001b[39m \n\u001b[32m   1965\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1994\u001b[39m \u001b[33;03m    Generated chat completion or a stream of chat completion chunks.\u001b[39;00m\n\u001b[32m   1995\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1996\u001b[39m handler = (\n\u001b[32m   1997\u001b[39m     \u001b[38;5;28mself\u001b[39m.chat_handler\n\u001b[32m   1998\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._chat_handlers.get(\u001b[38;5;28mself\u001b[39m.chat_format)\n\u001b[32m   1999\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m llama_chat_format.get_chat_completion_handler(\u001b[38;5;28mself\u001b[39m.chat_format)\n\u001b[32m   2000\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2001\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllama\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2007\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2008\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2010\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2011\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2012\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2013\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2014\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2015\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2016\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2017\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2018\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2019\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2020\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2022\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2023\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2024\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2025\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2026\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2028\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2031\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ishaq\\anaconda3\\envs\\llama_clean\\Lib\\site-packages\\llama_cpp\\llama_chat_format.py:662\u001b[39m, in \u001b[36mchat_formatter_to_chat_completion_handler.<locals>.chat_completion_handler\u001b[39m\u001b[34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs, **kwargs)\u001b[39m\n\u001b[32m    657\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e), file=sys.stderr)\n\u001b[32m    658\u001b[39m         grammar = llama_grammar.LlamaGrammar.from_string(\n\u001b[32m    659\u001b[39m             llama_grammar.JSON_GBNF, verbose=llama.verbose\n\u001b[32m    660\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m completion_or_chunks = \u001b[43mllama\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    688\u001b[39m     tool_name = tool[\u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ishaq\\anaconda3\\envs\\llama_clean\\Lib\\site-packages\\llama_cpp\\llama.py:1835\u001b[39m, in \u001b[36mLlama.create_completion\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1833\u001b[39m     chunks: Iterator[CreateCompletionStreamResponse] = completion_or_chunks\n\u001b[32m   1834\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[32m-> \u001b[39m\u001b[32m1835\u001b[39m completion: Completion = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1836\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ishaq\\anaconda3\\envs\\llama_clean\\Lib\\site-packages\\llama_cpp\\llama.py:1271\u001b[39m, in \u001b[36mLlama._create_completion\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1268\u001b[39m     \u001b[38;5;28mself\u001b[39m._ctx.reset_timings()\n\u001b[32m   1270\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_tokens) >= \u001b[38;5;28mself\u001b[39m._n_ctx:\n\u001b[32m-> \u001b[39m\u001b[32m1271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1272\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRequested tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompt_tokens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exceed context window of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllama_cpp.llama_n_ctx(\u001b[38;5;28mself\u001b[39m.ctx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1273\u001b[39m     )\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_tokens <= \u001b[32m0\u001b[39m:\n\u001b[32m   1276\u001b[39m     \u001b[38;5;66;03m# Unlimited, depending on n_ctx.\u001b[39;00m\n\u001b[32m   1277\u001b[39m     max_tokens = \u001b[38;5;28mself\u001b[39m._n_ctx - \u001b[38;5;28mlen\u001b[39m(prompt_tokens)\n",
      "\u001b[31mValueError\u001b[39m: Requested tokens (17091) exceed context window of 4096"
     ]
    }
   ],
   "source": [
    "response = llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a senior SOC analyst reviewing SIEM data.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_tokens=700,\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "print(\"Full Analysis:\\n\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796f2bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llama_clean)",
   "language": "python",
   "name": "llama_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
